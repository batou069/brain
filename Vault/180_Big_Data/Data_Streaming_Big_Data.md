---
tags:
  - big_data
  - data_streaming
  - real_time_processing
  - stream_processing
  - concept
aliases:
  - Streaming Data
  - Real-time Data Processing
related:
  - "[[_Big_Data_MOC]]"
  - "[[Big_Data_Definition_Characteristics]]"
  - "[[Apache_Kafka]]"
  - "[[Apache_Flink]]"
  - "[[Apache_Spark_MOC]]"
  - "[[MapReduce]]"
worksheet:
  - WS_BigData_1
date_created: 2025-06-09
---
# Data Streaming in Big Data

## Definition
**Data Streaming** (or **Stream Processing**) refers to the practice of processing data **continuously and in real-time (or near real-time)** as it is generated or received, rather than collecting it into batches for later processing. The data arrives as a continuous flow or "stream" of events or records.

This contrasts with traditional batch processing (like classic [[MapReduce|MapReduce]]), where data is first stored and then processed in large, discrete chunks.

>[!question] What is Data Streaming?
>Data Streaming is a data processing paradigm where data is treated as a continuous, unbounded sequence of events or records (a "stream") that are processed as they arrive. Instead of waiting for a complete dataset or a large batch, stream processing systems analyze and act upon data items individually or in small time windows, typically with low latency.
>
>Key characteristics:
>-   **Continuous Flow:** Data is not finite or bounded; it arrives indefinitely.
>-   **Real-time or Near Real-time:** Processing occurs with minimal delay after data generation or reception.
>-   **Event-driven:** Operations are often triggered by the arrival of new data events.
>-   **Small Units of Data:** Data is processed record by record or in small micro-batches/windows.
>-   **Stateful or Stateless Processing:** Operations can be stateless (each event processed independently) or stateful (maintaining and updating state based on past events, e.g., for aggregations over time windows).

## Why Data Streaming?
The need for data streaming arises from the **Velocity** aspect of [[Big_Data_Definition_Characteristics|Big Data]] and the increasing demand for immediate insights and actions:
-   **Real-time Analytics:** Gaining insights from live data to understand current conditions (e.g., website traffic, sensor readings, social media trends).
-   **Immediate Decision Making:** Enabling automated or human decisions based on the most up-to-date information (e.g., fraud detection, algorithmic trading, dynamic pricing).
-   **Monitoring and Alerting:** Continuously monitoring systems or events and triggering alerts when anomalies or specific conditions are detected (e.g., system health monitoring, intrusion detection).
-   **Personalization:** Providing real-time personalized experiences for users (e.g., content recommendations, targeted advertising).
-   **Internet of Things (IoT):** Processing massive streams of data generated by sensors and connected devices.

## Key Concepts in Stream Processing
[list2tab|#Stream Processing Concepts]
- Event
    -   A single data record or item in the stream (e.g., a tweet, a log entry, a sensor reading, a financial transaction).
- Stream
    -   An unbounded sequence of events ordered by time (either event time or processing time).
- Windowing
    -   Since streams are infinite, aggregations and some analyses are often performed over finite **windows** of data.
    -   **Tumbling Windows:** Fixed-size, non-overlapping time intervals (e.g., count events every 5 minutes).
    -   **Sliding Windows:** Fixed-size, overlapping time intervals (e.g., calculate a 5-minute moving average, updated every 1 minute).
    -   **Session Windows:** Group events based on periods of activity followed by inactivity (e.g., a user's session on a website).
- State Management
    -   Many stream processing applications require maintaining **state** (e.g., current counts, sums, user profiles) that is updated as new events arrive. Managing state reliably and consistently in a distributed stream processing system is a key challenge.
- Watermarks
    -   A mechanism to track the progress of event time in a stream, helping to deal with out-of-order events and determine when windows can be considered complete for processing.
- Processing Guarantees
    -   **At-most-once:** Each event is processed either once or not at all (data loss possible on failure).
    -   **At-least-once:** Each event is guaranteed to be processed, but could be processed multiple times on failure and recovery (requires idempotent operations or deduplication).
    -   **Exactly-once:** Each event is processed exactly once, effectively. This is the most desirable but often the hardest to achieve and can have performance overhead.
- Latency vs. Throughput
    -   **Latency:** The delay between an event occurring and it being processed/acted upon. Low latency is critical for real-time applications.
    -   **Throughput:** The rate at which events can be processed (e.g., events per second). High throughput is needed for high-velocity streams.
    -   Often a trade-off between the two.

## Common Stream Processing Architectures & Tools
-   **Message Queues / Brokers:**
    -   Systems like **[[Apache_Kafka|Apache Kafka]]**, RabbitMQ, AWS Kinesis Data Streams are used to ingest and buffer high-velocity data streams reliably before they are consumed by processing engines. They act as a durable, distributed log.
-   **Stream Processing Engines:**
    -   Frameworks designed to perform computations on data streams.
    -   **[[Apache_Flink|Apache Flink]]:** A true stream processing engine with strong support for event time processing, state management, and exactly-once semantics.
    -   **[[Apache_Spark_MOC|Apache Spark Streaming / Structured Streaming]]:** Processes streams as a sequence of micro-batches (Spark Streaming) or as a continuous table (Structured Streaming), providing a high-level API.
    -   **Apache Storm:** An older, low-latency stream processing framework.
    -   **ksqlDB / Kafka Streams:** Libraries for building stream processing applications directly with Kafka.
    -   Cloud-specific services: Google Cloud Dataflow, AWS Kinesis Data Analytics, Azure Stream Analytics.

## Example Use Cases
-   **Fraud Detection:** Analyzing financial transaction streams in real-time to identify and block fraudulent activities.
-   **Real-time Bidding (Ad Tech):** Processing bid requests and making decisions within milliseconds.
-   **IoT Sensor Data Analysis:** Monitoring sensor data from industrial equipment for predictive maintenance or from smart city devices for traffic management.
-   **Log Analysis and Anomaly Detection:** Processing application or system logs to detect errors, security breaches, or unusual patterns.
-   **Personalized Recommendations:** Updating user recommendations based on their latest interactions on a website or app.

Data streaming is becoming increasingly important as organizations seek to leverage the immediate value of continuously generated data.

---